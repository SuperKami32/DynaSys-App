{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuperKami32/DynaSys-App/blob/main/Apollo_TRAINER_0_90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XSRaX4ZQYqP",
        "outputId": "7b670cd0-9f3c-429a-a020-c64b5a7f2d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ rl_checkpoints ready — training will start fresh.\n",
            "\n",
            "=== Sweep 1/4: {'lr': 0.001, 'epsilon_decay': 0.999, 'target_update_freq': 200} ===\n",
            "[Train] Episodes=300 | Steps/ep=252\n",
            "Ep 5/300 | AvgR=-2721.05 | Bal=$13,595 | eps=0.328\n",
            "Ep 10/300 | AvgR=-2708.05 | Bal=$14,350 | eps=0.239\n",
            "Ep 15/300 | AvgR=-2616.97 | Bal=$12,902 | eps=0.175\n",
            "Ep 20/300 | AvgR=-2427.85 | Bal=$13,450 | eps=0.127\n",
            "Ep 25/300 | AvgR=-2229.28 | Bal=$13,612 | eps=0.093\n",
            "Ep 30/300 | AvgR=-2030.58 | Bal=$13,884 | eps=0.068\n",
            "Ep 35/300 | AvgR=-1850.24 | Bal=$13,975 | eps=0.049\n",
            "Ep 40/300 | AvgR=-1720.14 | Bal=$14,210 | eps=0.036\n",
            "Ep 45/300 | AvgR=-1611.23 | Bal=$13,670 | eps=0.026\n"
          ]
        }
      ],
      "source": [
        "# ======================= RL Portfolio Trainer v8.2 — Self‑Tuning =======================\n",
        "# Full, standalone trainer. Exports agent_weights.json for Engine v8.0 RL nudges.\n",
        "# Includes: dueling Double DQN with n-step replay, curriculum + adversaries,\n",
        "# committee signals, volatility-aware reward shaping, evaluation summary export,\n",
        "# and an auto-sweep hyperparameter tuner so you don't have to guess.\n",
        "\n",
        "import os, json, random, shutil, warnings\n",
        "from collections import deque\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "SEED = 42\n",
        "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "# --- Shared Save Directory (RL Trainer <-> Engine) ---\n",
        "SAVE_DIR = \"/content/rl_checkpoints\"\n",
        "RESET_CHECKPOINTS = True  # set False to keep prior models\n",
        "if RESET_CHECKPOINTS:\n",
        "    shutil.rmtree(SAVE_DIR, ignore_errors=True)\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "NUDGE_PATH = os.path.join(SAVE_DIR, \"agent_weights.json\")\n",
        "EVAL_PATH  = os.path.join(SAVE_DIR, \"evaluation_summary.json\")\n",
        "POLICY_PATH = os.path.join(SAVE_DIR, \"rl_policy_model.keras\")\n",
        "BEST_PATH   = os.path.join(SAVE_DIR, \"rl_policy_model_best.keras\")\n",
        "\n",
        "print(\"✅ rl_checkpoints ready — training will start fresh.\" if RESET_CHECKPOINTS else \"ℹ️ Using existing checkpoints.\")\n",
        "\n",
        "# -------------------- CONFIG --------------------\n",
        "CONFIG = {\n",
        "    # training loop\n",
        "    \"n_step\": 3,\n",
        "    \"episodes\": 300,\n",
        "    \"max_steps\": 252,\n",
        "    \"resume_from_checkpoint\": True,\n",
        "    \"checkpoint_name\": POLICY_PATH,\n",
        "    \"best_ckpt_name\":  BEST_PATH,\n",
        "\n",
        "    # DQN\n",
        "    \"epsilon_start\": 0.35,\n",
        "    \"epsilon_min\": 0.01,\n",
        "    \"epsilon_decay\": 0.999,\n",
        "    \"gamma\": 0.99,\n",
        "    \"lr\": 1e-3,\n",
        "    \"replay_capacity\": 50_000,\n",
        "    \"batch_size\": 64,\n",
        "    \"train_start\": 1_000,\n",
        "    \"train_freq\": 4,\n",
        "    \"target_update_freq\": 200,\n",
        "\n",
        "    # market realism\n",
        "    \"slippage_bps\": 2.0,\n",
        "    \"fee_bps\": 0.0,\n",
        "    \"reward_scale\": 100.0,\n",
        "    \"lambda_turnover\": 0.03,\n",
        "    \"lambda_drawdown\": 0.01,\n",
        "\n",
        "    # curriculum\n",
        "    \"eval_window\": 10,\n",
        "    \"patience\": 150,\n",
        "    \"curriculum_levels\": (1, 2),  # 1=Friction, 2=AlphaThief\n",
        "    \"rotate_every_ep\": 3,\n",
        "    \"ramp_ep\": 250,\n",
        "\n",
        "    # env realism\n",
        "    \"monthly_dca\": 300.0,\n",
        "    \"cash_monthly_apy\": 0.04,\n",
        "\n",
        "    # export scale\n",
        "    \"export_bias_scale\": 0.02,\n",
        "\n",
        "    # sweep (micro tuner)\n",
        "    \"sweep_configs\": [\n",
        "        {\"lr\":1e-3,  \"epsilon_decay\":0.999, \"target_update_freq\":200},\n",
        "        {\"lr\":7.5e-4,\"epsilon_decay\":0.999, \"target_update_freq\":300},\n",
        "        {\"lr\":5e-4,  \"epsilon_decay\":0.998, \"target_update_freq\":200},\n",
        "        {\"lr\":1e-3,  \"epsilon_decay\":0.997, \"target_update_freq\":400},\n",
        "    ],\n",
        "}\n",
        "\n",
        "# toggles\n",
        "CONFIG.update({\n",
        "    \"use_sentiment\": True,\n",
        "    \"sentiment_path\": os.path.join(SAVE_DIR, \"sentiment_today.json\"),\n",
        "    \"sentiment_fallback\": 0.0,\n",
        "    \"sentiment_vol_fallback\": 0.0,\n",
        "    \"use_benchmark_reward\": True,\n",
        "    \"benchmark_alloc\": (0.60, 0.35, 0.05),\n",
        "    \"alpha_reward_weight\": 0.75,\n",
        "    \"use_committee\": True,\n",
        "})\n",
        "\n",
        "CONFIG[\"rl_policy_path\"] = NUDGE_PATH\n",
        "CONFIG[\"weights_history_path\"] = os.path.join(SAVE_DIR, \"last_weights.json\")\n",
        "\n",
        "# ===================================================================================================\n",
        "# Committee Signals\n",
        "# ===================================================================================================\n",
        "def load_sentiment(path=CONFIG[\"sentiment_path\"], default=0.0, default_vol=0.0):\n",
        "    data = {} # Initialize data\n",
        "    try:\n",
        "        with open(path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            return {\"market\": float(data.get(\"market\", default)),\n",
        "                    \"vol\": float(data.get(\"vol\", default_vol))}\n",
        "    except:\n",
        "        return {\n",
        "    \"market\": float(data.get(\"market\", default)),\n",
        "    \"vol\": float(data.get(\"vol\", default_vol)),\n",
        "    \"policy\": float(data.get(\"policy\", 0.0))\n",
        "}\n",
        "\n",
        "\n",
        "class CommitteeSignals:\n",
        "    def _safe_mean(self, arr): return float(np.mean(arr)) if len(arr) else 0.0\n",
        "    def compute(self, growth_hist, income_hist, port_ret_hist, drawdown, roll_vol, sentiment):\n",
        "        mom = self._safe_mean(growth_hist[-10:]) - self._safe_mean(income_hist[-10:])\n",
        "        momentum_sig = float(np.tanh(mom * 50))\n",
        "        spread = self._safe_mean(growth_hist[-63:]) - self._safe_mean(income_hist[-63:])\n",
        "        value_sig = float(np.tanh(-spread * 30))\n",
        "        sent_sig = float(np.clip(sentiment.get(\"market\", 0.0), -1, 1))\n",
        "        risk = float(np.clip(0.5*drawdown + 0.5*np.tanh(roll_vol*20), 0, 1))\n",
        "        macro_sig = float(np.clip(sentiment.get(\"policy\", 0.0), -1, 1))\n",
        "        growth_tilt = float(np.clip(\n",
        "            0.45*momentum_sig + 0.25*sent_sig + 0.20*value_sig + 0.10*macro_sig,\n",
        "            -1, 1\n",
        "        ))\n",
        "\n",
        "        return {\"growth_tilt\": growth_tilt, \"cash_bias\": risk,\n",
        "                \"momentum_sig\": momentum_sig, \"value_sig\": value_sig,\n",
        "                \"sent_sig\": sent_sig, \"macro_sig\": macro_sig}\n",
        "\n",
        "# ===================================================================================================\n",
        "# Adversaries\n",
        "# ===================================================================================================\n",
        "class AdversaryBase:\n",
        "    def reset(self): pass\n",
        "    def step(self, state, action, info): return {}\n",
        "\n",
        "class FrictionAdversary(AdversaryBase):\n",
        "    def __init__(self, max_slip=12.0, max_fee=8.0, prob_spike=0.05, vol_boost=1.5):\n",
        "        self.max_slip, self.max_fee = max_slip, max_fee\n",
        "        self.prob_spike, self.vol_boost = prob_spike, vol_boost\n",
        "        self.cooldown = 0\n",
        "    def reset(self): self.cooldown = 0\n",
        "    def step(self, state, action, info):\n",
        "        prev_action = np.asarray(info.get(\"prev_action\", action), float)\n",
        "        turnover = float(np.sum(np.abs(np.asarray(action, float) - prev_action)))\n",
        "        slip = min(self.max_slip, 2.0 + 120.0 * turnover)\n",
        "        fee  = min(self.max_fee,  0.5 +  40.0 * turnover)\n",
        "        spike = (np.random.rand() < self.prob_spike) and (self.cooldown == 0)\n",
        "        if spike: self.cooldown = np.random.randint(5, 15)\n",
        "        else: self.cooldown = max(0, self.cooldown - 1)\n",
        "        return {\"slippage_bps\": slip, \"fee_bps\": fee,\n",
        "                \"vol_boost\": (self.vol_boost if self.cooldown > 0 else 1.0),\n",
        "                \"max_trade_cap\": 0.35}\n",
        "\n",
        "class AlphaThiefAdversary(AdversaryBase):\n",
        "    def __init__(self, strength=0.6): self.strength = strength\n",
        "    def step(self, state, action, info):\n",
        "        growth_tilt = float(action[0]) - 0.5\n",
        "        shock = - self.strength * 0.006 * np.sign(growth_tilt) * abs(growth_tilt) * 2.0\n",
        "        return {\"shock_ret_growth\": shock, \"max_trade_cap\": 0.30, \"vol_boost\": 1.2}\n",
        "\n",
        "def make_adversary(level, difficulty):\n",
        "    difficulty = float(np.clip(difficulty, 0, 1))\n",
        "    return (FrictionAdversary(8+12*difficulty, 3+7*difficulty, 0.02+0.10*difficulty, 1.0+0.7*difficulty)\n",
        "            if level==1 else AlphaThiefAdversary(0.3+0.7*difficulty))\n",
        "\n",
        "# ===================================================================================================\n",
        "# Environment\n",
        "# ===================================================================================================\n",
        "@dataclass\n",
        "class MarketStreams:\n",
        "    growth_rets: np.ndarray\n",
        "    income_rets: np.ndarray\n",
        "    cash_rets: np.ndarray\n",
        "    regimes: np.ndarray\n",
        "\n",
        "def _monthly_to_daily(mu_monthly): return (1 + mu_monthly)**(1/21) - 1\n",
        "\n",
        "def _regime_path(max_steps):\n",
        "    segs, remain, last = [], max_steps, None\n",
        "    while remain > 0:\n",
        "        seg_len = min(np.random.randint(30, 90), remain)\n",
        "        choices = [0,1,2] if last is None else [r for r in [0,1,2] if r != last]\n",
        "        regime = random.choice(choices)\n",
        "        segs.extend([regime]*seg_len)\n",
        "        last, remain = regime, remain - seg_len\n",
        "    return np.array(segs[:max_steps], dtype=int)\n",
        "\n",
        "def _toy_streams(max_steps):\n",
        "    regimes = _regime_path(max_steps)\n",
        "    mu_g_month = {2: 0.010, 1: 0.002, 0: -0.006}\n",
        "    mu_i_month = {2: 0.004, 1: 0.002, 0: -0.001}\n",
        "    sd_g_daily = {2: 0.015, 1: 0.010, 0: 0.020}\n",
        "    sd_i_daily = {2: 0.008, 1: 0.006, 0: 0.012}\n",
        "    g,i,c = [],[],[]\n",
        "    mu_c_daily = _monthly_to_daily(CONFIG[\"cash_monthly_apy\"]/12.0)\n",
        "    for r in regimes:\n",
        "        base = np.random.normal(0, 0.006)\n",
        "        mu_g = _monthly_to_daily(mu_g_month[r]); mu_i = _monthly_to_daily(mu_i_month[r])\n",
        "        g.append(np.random.normal(mu_g, sd_g_daily[r]) + 0.5*base)\n",
        "        i.append(np.random.normal(mu_i, sd_i_daily[r]) + 0.2*base)\n",
        "        c.append(mu_c_daily)\n",
        "    return MarketStreams(np.array(g), np.array(i), np.array(c), regimes)\n",
        "\n",
        "class PortfolioEnv:\n",
        "    def __init__(self, initial_balance=10_000.0, max_steps=252, adversary=None, committee=None,\n",
        "                 benchmark_alloc=CONFIG[\"benchmark_alloc\"]):\n",
        "        self.initial_balance = initial_balance\n",
        "        self.max_steps = max_steps\n",
        "        self.adversary = adversary or AdversaryBase()\n",
        "        self.committee = committee or (CommitteeSignals() if CONFIG[\"use_committee\"] else None)\n",
        "        self.bm_alloc = np.array(benchmark_alloc, float)\n",
        "        self.streams = _toy_streams(max_steps)\n",
        "        self.sentiment = load_sentiment() if CONFIG[\"use_sentiment\"] else {\"market\":0.0,\"vol\":0.0}\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = self.initial_balance\n",
        "        self.bench_balance = self.initial_balance\n",
        "        self.growth_alloc, self.income_alloc, self.cash_alloc = 0.5, 0.45, 0.05\n",
        "        self.prev_action = np.array([self.growth_alloc, self.income_alloc])\n",
        "        self.step_idx, self.daily_returns, self.portfolio_curve = 0, [], [self.balance]\n",
        "        self.g_hist, self.i_hist = [], []\n",
        "        self.max_equity, self.drawdown = self.balance, 0.0\n",
        "        self.last_committee = {\"growth_tilt\":0.0,\"cash_bias\":0.0,\"momentum_sig\":0.0,\"value_sig\":0.0,\"sent_sig\":0.0}\n",
        "        self.adversary.reset()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _committee_update(self, roll_vol):\n",
        "        if not self.committee: return\n",
        "        self.last_committee = self.committee.compute(np.array(self.g_hist[-63:], float),\n",
        "                                                     np.array(self.i_hist[-63:], float),\n",
        "                                                     np.array(self.daily_returns[-63:], float),\n",
        "                                                     self.drawdown, roll_vol, self.sentiment)\n",
        "\n",
        "    def _get_state(self):\n",
        "        r = np.array(self.daily_returns[-10:], float)\n",
        "        roll_ret, roll_vol = float(r.mean()) if r.size else 0.0, float(r.std()) if r.size>1 else 0.0\n",
        "        progress = self.step_idx / max(1, self.max_steps)\n",
        "        reg = self.streams.regimes[min(self.step_idx, self.max_steps-1)]\n",
        "        regime_oh = [1,0,0] if reg==2 else [0,1,0] if reg==1 else [0,0,1]\n",
        "        self._committee_update(roll_vol)\n",
        "        alpha_so_far = float(self.balance / max(self.bench_balance, 1e-9) - 1.0)\n",
        "        return np.array([\n",
        "            self.growth_alloc, self.income_alloc, self.cash_alloc,\n",
        "            roll_ret, roll_vol, progress, self.drawdown,\n",
        "            *regime_oh,\n",
        "            float(self.sentiment.get(\"market\", 0.0)),\n",
        "            float(self.sentiment.get(\"vol\", 0.0)),\n",
        "            self.last_committee[\"growth_tilt\"], self.last_committee[\"cash_bias\"],\n",
        "            self.last_committee[\"momentum_sig\"], self.last_committee[\"value_sig\"],\n",
        "            self.last_committee[\"sent_sig\"], alpha_so_far\n",
        "        ], np.float32)\n",
        "\n",
        "    def _apply_turnover_cap(self, old_alloc, target_alloc, cap_l1):\n",
        "        delta = target_alloc - old_alloc\n",
        "        l1 = np.sum(np.abs(delta))\n",
        "        return target_alloc if l1 <= cap_l1 or cap_l1<=0 else old_alloc + delta*(cap_l1/(l1+1e-8))\n",
        "\n",
        "    def step(self, action):\n",
        "        action = np.clip(action, 0, 1)\n",
        "        if action.sum()>1: action /= action.sum()\n",
        "        target_allocs = np.array([action[0], action[1], 1-(action[0]+action[1])])\n",
        "        old_alloc = np.array([self.growth_alloc, self.income_alloc, self.cash_alloc])\n",
        "        adv_eff = self.adversary.step(self._get_state(), action, {\"prev_action\": self.prev_action})\n",
        "        blended = old_alloc*0.8 + 0.2*target_allocs\n",
        "        blended = self._apply_turnover_cap(old_alloc, blended, adv_eff.get(\"max_trade_cap\", 1.0))\n",
        "        blended = blended/np.sum(blended)\n",
        "        turnover = float(np.sum(np.abs(blended-old_alloc)))\n",
        "        self.growth_alloc, self.income_alloc, self.cash_alloc = blended\n",
        "        self.prev_action = np.array([self.growth_alloc, self.income_alloc])\n",
        "        idx = min(self.step_idx, self.max_steps-1)\n",
        "        r_g = self.streams.growth_rets[idx]*adv_eff.get(\"vol_boost\",1.0)+adv_eff.get(\"shock_ret_growth\",0.0)\n",
        "        r_i = self.streams.income_rets[idx]*adv_eff.get(\"vol_boost\",1.0)\n",
        "        r_c = self.streams.cash_rets[idx]\n",
        "        self.g_hist.append(r_g); self.i_hist.append(r_i)\n",
        "        step_gross = self.growth_alloc*r_g+self.income_alloc*r_i+self.cash_alloc*r_c\n",
        "        bm_gross = float(self.bm_alloc[0]*r_g+self.bm_alloc[1]*r_i+self.bm_alloc[2]*r_c)\n",
        "        cost = (adv_eff.get(\"slippage_bps\", CONFIG[\"slippage_bps\"]) +\n",
        "                adv_eff.get(\"fee_bps\", CONFIG[\"fee_bps\"])) / 1e4 * turnover\n",
        "        step_ret = step_gross - cost\n",
        "        deposit = CONFIG[\"monthly_dca\"] if (self.step_idx % 21 == 0 and self.step_idx>0) else 0.0\n",
        "        self.balance = self.balance*(1+step_ret) + deposit\n",
        "        self.bench_balance = self.bench_balance*(1+bm_gross) + deposit\n",
        "        self.portfolio_curve.append(self.balance)\n",
        "        self.daily_returns.append(step_ret)\n",
        "        self.max_equity = max(self.max_equity, self.balance)\n",
        "        prev_dd = self.drawdown\n",
        "        self.drawdown = 1 - self.balance/self.max_equity\n",
        "\n",
        "        rew = CONFIG[\"reward_scale\"]*step_ret - CONFIG[\"lambda_turnover\"]*turnover \\\n",
        "              - CONFIG[\"lambda_drawdown\"]*max(0, self.drawdown - prev_dd)*100.0\n",
        "        if CONFIG[\"use_benchmark_reward\"]:\n",
        "            rew += CONFIG[\"alpha_reward_weight\"]*CONFIG[\"reward_scale\"]*(step_ret - bm_gross)\n",
        "        if self.cash_alloc > 0.7: rew -= 0.05 * CONFIG[\"reward_scale\"]\n",
        "        elif self.cash_alloc < 0.3: rew += 0.01 * CONFIG[\"reward_scale\"]\n",
        "        rew -= 0.25 * np.sum(np.square(target_allocs - old_alloc)) * CONFIG[\"reward_scale\"]\n",
        "        rew = np.clip(rew, -500, 500)\n",
        "        rew = np.tanh(rew / 200.0) * 200.0\n",
        "\n",
        "        self.step_idx += 1\n",
        "        done = self.step_idx >= self.max_steps\n",
        "        return self._get_state(), float(rew), done, {}\n",
        "\n",
        "# ===================================================================================================\n",
        "# Dueling Double DQN Agent with n-step replay\n",
        "# ===================================================================================================\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size, self.action_size = state_size, action_size\n",
        "        self.gamma, self.lr = CONFIG[\"gamma\"], CONFIG[\"lr\"]\n",
        "        self.n_step = CONFIG[\"n_step\"]; self.gamma_n = self.gamma ** self.n_step\n",
        "        self.nbuf = deque(maxlen=self.n_step)\n",
        "        self.epsilon, self.epsilon_min, self.epsilon_decay = (\n",
        "            CONFIG[\"epsilon_start\"], CONFIG[\"epsilon_min\"], CONFIG[\"epsilon_decay\"]\n",
        "        )\n",
        "        self.memory = deque(maxlen=CONFIG[\"replay_capacity\"])\n",
        "        self.q_network = self._build_model()\n",
        "        self.target_network = self._build_model()\n",
        "        self.update_target_network(tau=1.0)\n",
        "        self.ckpt_path, self.best_ckpt = CONFIG[\"checkpoint_name\"], CONFIG[\"best_ckpt_name\"]\n",
        "        if CONFIG[\"resume_from_checkpoint\"] and os.path.exists(self.ckpt_path):\n",
        "            try:\n",
        "                self.q_network = keras.models.load_model(self.ckpt_path)\n",
        "                self.target_network = keras.models.load_model(self.ckpt_path)\n",
        "                print(f\"[Resume] Loaded checkpoint from {self.ckpt_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[Resume] Failed: {e}\")\n",
        "\n",
        "    def _build_model(self):\n",
        "        inp = keras.layers.Input(shape=(self.state_size,))\n",
        "        x = keras.layers.Dense(192, activation=\"relu\")(inp)\n",
        "        x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "        x = keras.layers.Dense(64,  activation=\"relu\")(x)\n",
        "        V = keras.layers.Dense(1, activation=\"linear\")(keras.layers.Dense(32, activation=\"relu\")(x))\n",
        "        A = keras.layers.Dense(self.action_size, activation=\"linear\")(keras.layers.Dense(32, activation=\"relu\")(x))\n",
        "        A_mean = keras.layers.Lambda(lambda a: keras.backend.mean(a, axis=1, keepdims=True))(A)\n",
        "        Q = keras.layers.Add()([V, keras.layers.Subtract()([A, A_mean])])\n",
        "        m = keras.Model(inputs=inp, outputs=Q)\n",
        "        m.compile(optimizer=keras.optimizers.Adam(learning_rate=self.lr), loss=\"mse\")\n",
        "        return m\n",
        "\n",
        "    def update_target_network(self, tau=0.005):\n",
        "        new_weights = self.q_network.get_weights()\n",
        "        tgt_weights = self.target_network.get_weights()\n",
        "        if not tgt_weights:\n",
        "            self.target_network.set_weights(new_weights); return\n",
        "        updated = [tau*n + (1-tau)*t for n,t in zip(new_weights, tgt_weights)]\n",
        "        self.target_network.set_weights(updated)\n",
        "\n",
        "    def remember(self, s, a, r, ns, d):\n",
        "        self.nbuf.append((s, a, r, ns, d))\n",
        "        if len(self.nbuf) < self.n_step: return\n",
        "        R, next_state, done_flag = 0.0, ns, d\n",
        "        for i, (_, _, r_i, ns_i, d_i) in enumerate(self.nbuf):\n",
        "            R += (self.gamma ** i) * r_i\n",
        "            if d_i:\n",
        "                done_flag, next_state = True, ns_i\n",
        "                break\n",
        "        state_0, action_0 = self.nbuf[0][0], self.nbuf[0][1]\n",
        "        self.memory.append((state_0, action_0, R, next_state, done_flag))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            a = np.random.rand(self.action_size)\n",
        "            return a/a.sum() if a.sum()>1 else a\n",
        "        return self.q_network.predict(state.reshape(1,-1), verbose=0)[0]\n",
        "\n",
        "    def replay(self, step_no):\n",
        "        if len(self.memory) < CONFIG[\"train_start\"] or step_no % CONFIG[\"train_freq\"] != 0:\n",
        "            return\n",
        "        batch = random.sample(self.memory, min(CONFIG[\"batch_size\"], len(self.memory)))\n",
        "        states  = np.array([b[0] for b in batch], np.float32)\n",
        "        rewards = np.array([b[2] for b in batch], np.float32)\n",
        "        next_st = np.array([b[3] for b in batch], np.float32)\n",
        "        dones   = np.array([b[4] for b in batch], bool)\n",
        "        q_vals = self.q_network.predict(states, verbose=0)\n",
        "        q_next_online = self.q_network.predict(next_st, verbose=0)\n",
        "        q_next_target = self.target_network.predict(next_st, verbose=0)\n",
        "        best_next = np.argmax(q_next_online, axis=1)\n",
        "        v_next = q_next_target[np.arange(q_next_target.shape[0]), best_next]\n",
        "        y = rewards + (1 - dones.astype(np.float32)) * (self.gamma ** CONFIG[\"n_step\"]) * v_next\n",
        "        targets = q_vals.copy()\n",
        "        for i in range(targets.shape[1]):\n",
        "            targets[:, i] = y\n",
        "        self.q_network.train_on_batch(states, targets)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def save_checkpoint(self, best=False):\n",
        "        self.q_network.save(self.best_ckpt if best else self.ckpt_path, include_optimizer=True)\n",
        "\n",
        "# ===================================================================================================\n",
        "# Curriculum & Training\n",
        "# ===================================================================================================\n",
        "def adversary_for_episode(ep_idx):\n",
        "    d = min(1.0, ep_idx / max(1, CONFIG[\"ramp_ep\"]))\n",
        "    choices = CONFIG[\"curriculum_levels\"]\n",
        "    lvl = choices[(ep_idx // CONFIG[\"rotate_every_ep\"]) % len(choices)]\n",
        "    return make_adversary(lvl, d)\n",
        "\n",
        "def train_agent(episodes=CONFIG[\"episodes\"], max_steps=CONFIG[\"max_steps\"]):\n",
        "    env0 = PortfolioEnv(max_steps=max_steps, adversary=AdversaryBase(), committee=CommitteeSignals())\n",
        "    agent = DQNAgent(env0._get_state().shape[0], action_size=2)\n",
        "    rewards_hist, balances_hist = [], []\n",
        "    best_score, since_improve, steps_done = -1e9, 0, 0\n",
        "    print(f\"[Train] Episodes={episodes} | Steps/ep={max_steps}\")\n",
        "    for ep in range(episodes):\n",
        "        env = PortfolioEnv(max_steps=max_steps, adversary=adversary_for_episode(ep), committee=CommitteeSignals())\n",
        "        s, ep_reward = env.reset(), 0.0\n",
        "        for t in range(max_steps):\n",
        "            a = agent.act(s)\n",
        "            ns, r, done, _ = env.step(a)\n",
        "            agent.remember(s, a, r, ns, done)\n",
        "            steps_done += 1\n",
        "            agent.replay(steps_done)\n",
        "            if steps_done % CONFIG[\"target_update_freq\"] == 0:\n",
        "                agent.update_target_network()\n",
        "            s, ep_reward = ns, ep_reward + r\n",
        "            if done: break\n",
        "        rewards_hist.append(ep_reward); balances_hist.append(env.balance)\n",
        "        avg_r = float(np.mean(rewards_hist[-CONFIG[\"eval_window\"]:]))\n",
        "        improved = avg_r > best_score; best_score = max(best_score, avg_r)\n",
        "        since_improve = 0 if improved else since_improve + 1\n",
        "        agent.save_checkpoint(best=improved); agent.save_checkpoint(best=False)\n",
        "        if (ep+1)%5==0:\n",
        "            print(f\"Ep {ep+1}/{episodes} | AvgR={avg_r:.2f} | Bal=${env.balance:,.0f} | eps={agent.epsilon:.3f}\")\n",
        "        if since_improve >= CONFIG[\"patience\"]:\n",
        "            print(\"[EarlyStop] No improvement.\"); break\n",
        "    # quick plots\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(1,2,1); plt.plot(rewards_hist); plt.title(\"Reward Curve\"); plt.grid()\n",
        "    plt.subplot(1,2,2); plt.plot(balances_hist); plt.title(\"Balance\"); plt.grid()\n",
        "    plt.tight_layout(); plt.show()\n",
        "    # evaluation snapshot\n",
        "    avg_recent_reward = float(np.mean(rewards_hist[-CONFIG[\"eval_window\"]:])) if rewards_hist else 0.0\n",
        "    with open(EVAL_PATH, \"w\") as f:\n",
        "        json.dump({\"avg_recent_reward\": avg_recent_reward,\n",
        "                   \"mean_balance\": float(np.mean(balances_hist)) if balances_hist else 0.0}, f, indent=2)\n",
        "    return agent, rewards_hist, balances_hist, avg_recent_reward\n",
        "\n",
        "# ===================================================================================================\n",
        "# Test, Evaluate, Export\n",
        "# ===================================================================================================\n",
        "def test_agent(agent, max_steps=CONFIG[\"max_steps\"]):\n",
        "    env = PortfolioEnv(max_steps=max_steps, adversary=AdversaryBase())\n",
        "    agent.epsilon = 0.0; s = env.reset()\n",
        "    for _ in range(max_steps):\n",
        "        a = agent.act(s); s,_,done,_=env.step(a)\n",
        "        if done: break\n",
        "    print(f\"Final Balance: ${env.balance:,.2f}\")\n",
        "    plt.figure(figsize=(8,4)); plt.plot(env.portfolio_curve); plt.title(\"Equity Curve\"); plt.grid(); plt.show()\n",
        "    return env\n",
        "\n",
        "def evaluate_agent(agent, n_runs=25, max_steps=CONFIG[\"max_steps\"], save_summary=True):\n",
        "    balances, returns, sharpes = [], [], []\n",
        "    for run in range(n_runs):\n",
        "        np.random.seed(10_000+run)\n",
        "        env = PortfolioEnv(max_steps=max_steps, adversary=AdversaryBase()); agent.epsilon=0.0\n",
        "        s = env.reset()\n",
        "        for _ in range(max_steps):\n",
        "            a=agent.act(s); s,_,done,_=env.step(a)\n",
        "            if done: break\n",
        "        balances.append(env.balance); returns.append((env.balance/env.initial_balance)-1)\n",
        "        arr=np.array(env.daily_returns); sharpe=np.mean(arr)/(np.std(arr)+1e-8)*np.sqrt(252) if arr.size>2 else 0.0\n",
        "        sharpes.append(sharpe)\n",
        "    summary={\"mean_final_balance\":float(np.mean(balances)),\n",
        "             \"median_final_balance\":float(np.median(balances)),\n",
        "             \"p10_balance\":float(np.percentile(balances,10)),\n",
        "             \"p90_balance\":float(np.percentile(balances,90)),\n",
        "             \"mean_total_return\":float(np.mean(returns)),\n",
        "             \"mean_sharpe\":float(np.mean(sharpes)),\n",
        "             \"pct_profitable_runs\":float(np.mean(np.array(returns)>0)*100)}\n",
        "    if save_summary:\n",
        "        with open(EVAL_PATH,\"w\") as f: json.dump(summary,f,indent=2)\n",
        "        print(f\"[Saved] Evaluation summary to {EVAL_PATH}\")\n",
        "    return summary\n",
        "\n",
        "def export_nudges_from_agent(agent, growth_tickers, income_tickers, out_path=NUDGE_PATH,\n",
        "                             bias_scale=CONFIG[\"export_bias_scale\"], probe_runs=50, max_steps=84):\n",
        "    agent.epsilon=0.0; growth_w,income_w=[],[]\n",
        "    for run in range(probe_runs):\n",
        "        np.random.seed(100_000+run)\n",
        "        env=PortfolioEnv(max_steps=max_steps,adversary=AdversaryBase())\n",
        "        s=env.reset()\n",
        "        for _ in range(max_steps):\n",
        "            a=agent.act(s); s,_,done,_=env.step(a)\n",
        "            if done: break\n",
        "        growth_w.append(env.growth_alloc); income_w.append(env.income_alloc)\n",
        "    g_bias,i_bias=float(np.mean(growth_w)-0.5),float(np.mean(income_w)-0.5)\n",
        "    g_nudge=float(np.clip(bias_scale*g_bias,-0.05,0.05)); i_nudge=float(np.clip(bias_scale*i_bias,-0.05,0.05))\n",
        "    nudges={**{t:g_nudge for t in growth_tickers},**{t:i_nudge for t in income_tickers}}\n",
        "    with open(out_path,\"w\") as f: json.dump(nudges,f,indent=2)\n",
        "    print(f\"[Export] RL nudges → {out_path}\\n\"+json.dumps(nudges,indent=2))\n",
        "    return nudges\n",
        "\n",
        "# ===================================================================================================\n",
        "# Auto-Sweep (Self-Tuning) — tries small variations and keeps best\n",
        "# ===================================================================================================\n",
        "def run_sweep():\n",
        "    best_score, best_cfg, best_agent = -1e9, None, None\n",
        "    base = {k:v for k,v in CONFIG.items()}  # shallow copy\n",
        "    for i, cfg in enumerate(CONFIG[\"sweep_configs\"], 1):\n",
        "        CONFIG.update({\"lr\": cfg[\"lr\"], \"epsilon_decay\": cfg[\"epsilon_decay\"],\n",
        "                       \"target_update_freq\": cfg[\"target_update_freq\"]})\n",
        "        print(f\"\\n=== Sweep {i}/{len(CONFIG['sweep_configs'])}: {cfg} ===\")\n",
        "        agent, rewards, balances, avg_recent = train_agent()\n",
        "        try:\n",
        "            with open(EVAL_PATH, \"r\") as f:\n",
        "                eval_score = float(json.load(f).get(\"avg_recent_reward\", avg_recent))\n",
        "        except Exception:\n",
        "            eval_score = avg_recent\n",
        "        print(f\"Sweep {i} score={eval_score:.3f}\")\n",
        "        if eval_score > best_score:\n",
        "            best_score, best_cfg, best_agent = eval_score, dict(cfg), agent\n",
        "            agent.save_checkpoint(best=True)\n",
        "    CONFIG.update(base)\n",
        "    if best_cfg:\n",
        "        CONFIG.update({\"lr\": best_cfg[\"lr\"], \"epsilon_decay\": best_cfg[\"epsilon_decay\"],\n",
        "                       \"target_update_freq\": best_cfg[\"target_update_freq\"]})\n",
        "    print(f\"\\nBest sweep score: {best_score:.3f} with {best_cfg}\")\n",
        "    return best_agent, best_cfg, best_score\n",
        "\n",
        "# ===================================================================================================\n",
        "# Main Run (wrapped into callable function)\n",
        "# ===================================================================================================\n",
        "def train_rl_agent(run_sweep_flag=True, export_nudges_flag=True):\n",
        "    if run_sweep_flag:\n",
        "        agent, best_cfg, best_score = run_sweep()\n",
        "    else:\n",
        "        agent, _, _ = train_agent()\n",
        "\n",
        "    # Evaluate and save summary\n",
        "    evaluate_agent(agent, n_runs=25)\n",
        "\n",
        "    # Export nudges if requested\n",
        "    if export_nudges_flag:\n",
        "        growth_list = [\"QQQM\",\"VTI\",\"AVUV\",\"AVDV\",\"VGT\",\"NVDA\",\"AAPL\",\"MSFT\",\"TSLA\",\"AMZN\",\"TQQQ\",\"SOXL\",\n",
        "               \"BTC-USD\",\"ETH-USD\",\"SOL-USD\",\"LINK-USD\",\"ADA-USD\",\"DOT-USD\",\"AVAX-USD\",\"UNI-USD\",\"AAVE-USD\"]\n",
        "        income_list = [\"SCHD\",\"O\",\"JEPI\",\"JEPQ\",\"DGRO\",\"VIG\",\"DIVO\",\"QYLD\",\"XYLD\",\"BIL\",\"TLT\"]\n",
        "\n",
        "        export_nudges_from_agent(agent, growth_list, income_list)\n",
        "\n",
        "    return {\n",
        "        \"nudges_path\": NUDGE_PATH,\n",
        "        \"eval_summary_path\": EVAL_PATH\n",
        "    }\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    results = train_rl_agent()\n",
        "    print(json.dumps(results, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmpACaHhR5RM"
      },
      "outputs": [],
      "source": [
        "# === RL Meta-Loop: safe auto-suggester + optional auto-rerun ===\n",
        "# Paste this AFTER you run the RL trainer once so evaluation_summary.json exists.\n",
        "# It will read the evaluation summary, propose a small config mutation,\n",
        "# and optionally run the trainer again in a safe, versioned temp folder.\n",
        "#\n",
        "# Behaviors:\n",
        "#  - dry_run=True : only propose suggestions, NO training launched.\n",
        "#  - auto_accept=False : will NOT replace best model automatically; it archives, prints diff, waits for you to set auto_accept True to update.\n",
        "#  - Keeps a 'meta_archive' with all tried configs and evals.\n",
        "#\n",
        "# IMPORTANT: adapt TRAIN_CMD if you run trainer differently (e.g., as module or different path).\n",
        "\n",
        "import os, json, time, shutil, subprocess, copy, datetime, hashlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# === User tunables ===\n",
        "RL_CHECKPOINT_DIR = Path(\"/content/rl_checkpoints\")           # where trainer writes evaluation_summary.json, agent_weights.json\n",
        "TRAINER_SCRIPT     = Path(\"/mnt/data/RL_Portfolio_Trainer_v7_0.py\")  # path to the trainer script (or change to your %run target)\n",
        "META_ARCHIVE_DIR   = RL_CHECKPOINT_DIR / \"meta_archive\"      # stores configs, evals, models from meta-loop\n",
        "BEST_ARCHIVE_DIR   = RL_CHECKPOINT_DIR / \"best_archive\"      # stores copy of the best model(s)\n",
        "DRY_RUN            = True     # True => only propose, do not retrain\n",
        "AUTO_ACCEPT        = False    # True => automatically accept & archive improved models\n",
        "MAX_GENERATIONS    = 6        # how many generations to attempt\n",
        "SEED_BASE          = 424242   # base seed (incremented per generation)\n",
        "MAX_RUNS_PER_GEN   = 1        # how many trainer runs to execute per generation (usually 1)\n",
        "TIMEOUT_SECONDS    = None     # if desired, you can set an overall timeout for trainer subprocesses\n",
        "\n",
        "# Create directories\n",
        "META_ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BEST_ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EVAL_PATH = RL_CHECKPOINT_DIR / \"evaluation_summary.json\"\n",
        "CONFIG_PATH = RL_CHECKPOINT_DIR / \"config_snapshot.json\"\n",
        "\n",
        "def read_json(path):\n",
        "    try:\n",
        "        with open(path, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def write_json(obj, path):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2, default=str)\n",
        "\n",
        "def timestamp():\n",
        "    return datetime.datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "def hash_dict(d):\n",
        "    s = json.dumps(d, sort_keys=True)\n",
        "    return hashlib.sha1(s.encode()).hexdigest()[:8]\n",
        "\n",
        "# Simple mutation policy: smart tiny changes around lr, epsilon_decay, target_update_freq, batch_size\n",
        "def propose_mutations(cfg):\n",
        "    base = copy.deepcopy(cfg)\n",
        "    # candidate knobs and sensible multiplicative ranges\n",
        "    knobs = {\n",
        "        \"lr\": {\"mult\": (0.6, 1.6)},\n",
        "        \"epsilon_decay\": {\"add\": (-0.002, 0.002), \"clip\": (0.990, 0.99995)},\n",
        "        \"target_update_freq\": {\"mult\": (0.7, 1.6), \"int\": True, \"clip\": (50, 2000)},\n",
        "        \"batch_size\": {\"mult\": (0.8, 1.5), \"int\": True, \"clip\": (16, 1024)},\n",
        "        \"replay_capacity\": {\"mult\": (0.7, 1.6), \"int\": True, \"clip\": (1000, 500000)},\n",
        "        \"n_step\": {\"add\": (-1, 2), \"int\": True, \"clip\": (1, 8)}\n",
        "    }\n",
        "\n",
        "    # Use small gaussian/random nudges, biased by performance if possible\n",
        "    new = copy.deepcopy(base)\n",
        "    for k, meta in knobs.items():\n",
        "        if k not in base:\n",
        "            continue\n",
        "        cur = base[k]\n",
        "        if \"mult\" in meta:\n",
        "            mn, mx = meta[\"mult\"]\n",
        "            factor = float(np.exp(np.random.uniform(np.log(mn), np.log(mx))))\n",
        "            cand = cur * factor\n",
        "        elif \"add\" in meta:\n",
        "            a, b = meta[\"add\"]\n",
        "            cand = cur + float(np.random.uniform(a, b))\n",
        "        else:\n",
        "            cand = cur\n",
        "        if meta.get(\"int\", False):\n",
        "            cand = int(max(meta.get(\"clip\", (None, None))[0] or cand, 1, round(cand)))\n",
        "        if \"clip\" in meta:\n",
        "            cmin, cmax = meta[\"clip\"]\n",
        "            if cmin is not None: cand = max(cmin, cand)\n",
        "            if cmax is not None: cand = min(cmax, cand)\n",
        "        new[k] = cand\n",
        "\n",
        "    # small sanity fixes\n",
        "    if \"epsilon_decay\" in new and new[\"epsilon_decay\"] > 0.99998:\n",
        "      new[\"epsilon_decay\"] = min(new[\"epsilon_decay\"], 0.99998)\n",
        "    return new\n",
        "\n",
        "def score_from_eval(eval_json):\n",
        "    if not eval_json:\n",
        "        return -1e9\n",
        "    # prefer avg_recent_reward if present, else mean_balance, else mean_total_return\n",
        "    score = eval_json.get(\"avg_recent_reward\")\n",
        "    if score is None:\n",
        "        score = eval_json.get(\"mean_total_return\")\n",
        "    if score is None:\n",
        "        score = eval_json.get(\"mean_balance\")\n",
        "    try:\n",
        "        return float(score)\n",
        "    except:\n",
        "        return -1e9\n",
        "\n",
        "def safe_copy_best_checkpoint(dst_dir, src_checkpoint_path_candidates):\n",
        "    \"\"\"\n",
        "    src_checkpoint_path_candidates: list of possible file paths (strings) the trainer may have saved the model to\n",
        "    This function copies any that exist into dst_dir with a timestamped name.\n",
        "    \"\"\"\n",
        "    copied = []\n",
        "    dst_dir = Path(dst_dir)\n",
        "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for p in src_checkpoint_path_candidates:\n",
        "        p = Path(p)\n",
        "        if p.exists():\n",
        "            name = f\"{p.name.split('.')[0]}_{timestamp()}_{hashlib.sha1(str(p).encode()).hexdigest()[:6]}\"\n",
        "            dst = dst_dir / name\n",
        "            try:\n",
        "                if p.is_dir():\n",
        "                    shutil.copytree(p, dst)\n",
        "                else:\n",
        "                    shutil.copy2(p, dst)\n",
        "                copied.append(str(dst))\n",
        "            except Exception as e:\n",
        "                print(\"Copy failed:\", e)\n",
        "    return copied\n",
        "\n",
        "# === Load current config & eval ===\n",
        "raw_cfg = read_json(CONFIG_PATH) or {}\n",
        "raw_eval = read_json(EVAL_PATH) or {}\n",
        "print(\"Loaded current CONFIG snapshot:\", str(CONFIG_PATH), \"exists?\", CONFIG_PATH.exists())\n",
        "print(\"Loaded last evaluation:\", str(EVAL_PATH), \"exists?\", EVAL_PATH.exists())\n",
        "\n",
        "current_score = score_from_eval(raw_eval)\n",
        "print(\"Current run score:\", current_score, \"raw eval keys:\", list(raw_eval.keys()) if raw_eval else [])\n",
        "\n",
        "# Meta-loop: iterate, propose, (optionally) run trainer, record results\n",
        "best_seen_score = current_score\n",
        "best_seen_eval = raw_eval\n",
        "best_cfg = raw_cfg\n",
        "best_archive_items = []\n",
        "\n",
        "for gen in range(1, MAX_GENERATIONS + 1):\n",
        "    print(\"\\n=== META GEN\", gen, \"===\")\n",
        "    # 1) Propose a new config mutation\n",
        "    proposal_cfg = propose_mutations(raw_cfg)\n",
        "    proposal_cfg[\"_meta\"] = {\n",
        "        \"generated_at\": timestamp(),\n",
        "        \"generation\": gen,\n",
        "        \"parent_score\": current_score,\n",
        "        \"parent_hash\": hash_dict(raw_cfg)\n",
        "    }\n",
        "    proposal_id = f\"gen{gen}_{timestamp()}_{hash_dict(proposal_cfg)}\"\n",
        "    proposal_folder = META_ARCHIVE_DIR / proposal_id\n",
        "    proposal_folder.mkdir(parents=True, exist_ok=True)\n",
        "    write_json(proposal_cfg, proposal_folder / \"proposed_config.json\")\n",
        "    print(\"Proposed config saved to\", str(proposal_folder / \"proposed_config.json\"))\n",
        "\n",
        "    # 2) Dry-run only?\n",
        "    if DRY_RUN:\n",
        "        print(\"DRY_RUN is True -> not launching trainer. Inspect proposed_config.json in meta_archive.\")\n",
        "        # Update raw_cfg to be the proposal for next iteration only if you want progressive proposals\n",
        "        raw_cfg = proposal_cfg\n",
        "        continue\n",
        "\n",
        "    # 3) If not dry-run, we will run the trainer in a temporary fresh workspace\n",
        "    #    to avoid clobbering current rl_checkpoints (trainer will still write into RL_CHECKPOINT_DIR by default).\n",
        "    #    We copy the proposed config into RL_CHECKPOINT_DIR as 'config_snapshot.json' before launching trainer.\n",
        "    write_json(proposal_cfg, RL_CHECKPOINT_DIR / \"config_snapshot.json\")\n",
        "    print(\"Wrote proposed config into RL checkpoint dir for trainer to pick up.\")\n",
        "\n",
        "    # 4) Launch the trainer subprocess. Modify TRAIN_CMD to your environment if needed.\n",
        "    #    We call the script directly with Python; if you run trainer via %run in the same notebook,\n",
        "    #    you can instead `exec(open(TRAINER_SCRIPT).read(), globals())` but subprocess keeps isolation.\n",
        "    TRAIN_CMD = [ \"python3\", str(TRAINER_SCRIPT) ]\n",
        "    run_results = None\n",
        "    try:\n",
        "        for run_idx in range(1, MAX_RUNS_PER_GEN + 1):\n",
        "            print(f\"Launching trainer run {run_idx}/{MAX_RUNS_PER_GEN} ...\")\n",
        "            start_t = time.time()\n",
        "            proc = subprocess.run(TRAIN_CMD, cwd=str(RL_CHECKPOINT_DIR), timeout=TIMEOUT_SECONDS)\n",
        "            elapsed = time.time() - start_t\n",
        "            print(\"Trainer finished (exitcode {}) in {:.1f}s\".format(proc.returncode, elapsed))\n",
        "            # 5) Load evaluation and snapshot\n",
        "            eval_after = read_json(EVAL_PATH)\n",
        "            write_json(eval_after or {}, proposal_folder / f\"evaluation_after_run{run_idx}.json\")\n",
        "            # copy any checkpoints\n",
        "            copied = safe_copy_best_checkpoint(proposal_folder / \"checkpoints\", [RL_CHECKPOINT_DIR / \"rl_policy_model_best.keras\", RL_CHECKPOINT_DIR / \"rl_policy_model.keras\"])\n",
        "            proposal_folder.joinpath(\"train_result_info.json\").write_text(json.dumps({\"exitcode\": proc.returncode, \"elapsed_s\": elapsed}))\n",
        "            run_results = {\"eval\": eval_after, \"copied_checkpoints\": copied, \"exitcode\": proc.returncode, \"elapsed\": elapsed}\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"Trainer timed out for proposal\", proposal_id)\n",
        "        write_json({\"timeout\": True}, proposal_folder / \"train_result_info.json\")\n",
        "        run_results = {\"eval\": None, \"timeout\": True}\n",
        "    except Exception as e:\n",
        "        print(\"Trainer run failed:\", e)\n",
        "        write_json({\"error\": str(e)}, proposal_folder / \"train_result_info.json\")\n",
        "        run_results = {\"eval\": None, \"error\": str(e)}\n",
        "\n",
        "    # 6) Scoring the run and archiving\n",
        "    eval_after = run_results.get(\"eval\") if run_results else None\n",
        "    score_after = score_from_eval(eval_after)\n",
        "    write_json({\"score_before\": current_score, \"score_after\": score_after}, proposal_folder / \"score_snapshot.json\")\n",
        "    print(\"Score before:\", current_score, \"score after:\", score_after)\n",
        "\n",
        "    # 7) If improved, archive model & optionally accept as new baseline\n",
        "    improved = score_after > best_seen_score\n",
        "    if improved:\n",
        "        print(\"=== IMPROVEMENT detected! ===\")\n",
        "        # copy candidate checkpoints to best archive\n",
        "        copied = safe_copy_best_checkpoint(BEST_ARCHIVE_DIR, [RL_CHECKPOINT_DIR / \"rl_policy_model_best.keras\", RL_CHECKPOINT_DIR / \"rl_policy_model.keras\"])\n",
        "        best_archive_items.append({\"gen\": gen, \"proposal\": proposal_id, \"score\": score_after, \"copied\": copied})\n",
        "        write_json({\"gen\": gen, \"score\": score_after, \"proposal\": proposal_id, \"copied\": copied}, BEST_ARCHIVE_DIR / f\"improve_{proposal_id}.json\")\n",
        "        best_seen_score = score_after\n",
        "        best_seen_eval = eval_after\n",
        "        best_cfg = proposal_cfg\n",
        "        # auto accept?\n",
        "        if AUTO_ACCEPT:\n",
        "            print(\"AUTO_ACCEPT is True -> accepting proposal as new baseline (copying into rl_checkpoints).\")\n",
        "            write_json(proposal_cfg, RL_CHECKPOINT_DIR / \"config_snapshot.json\")\n",
        "        else:\n",
        "            print(\"AUTO_ACCEPT is False -> NOT accepting automatically. To accept, set AUTO_ACCEPT=True or copy the config manually from archive.\")\n",
        "    else:\n",
        "        print(\"No improvement detected for this proposal.\")\n",
        "\n",
        "    # 8) update raw_cfg for the next generation -- optionally we can accept the proposal as parent or keep parent\n",
        "    # Choose to set parent to best_cfg so mutations are anchored to best-known config\n",
        "    raw_cfg = best_cfg\n",
        "\n",
        "    # Optional: short cooldown to let logs settle\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"\\n=== Meta-loop complete ===\")\n",
        "print(\"Best seen score:\", best_seen_score)\n",
        "print(\"Best seen eval snapshot stored in best_archive:\", list(BEST_ARCHIVE_DIR.glob(\"*\"))[:5])\n",
        "print(\"Meta archive folder:\", META_ARCHIVE_DIR)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvXxSi4xOjpjtE9p/H405U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}